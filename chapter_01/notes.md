# Understanding Large Language Models

## About
- High-level overview to fundamental of LLMs.
- Plan to build an LLM from scratch.

## Introduction
- "LLMs understand" : LLMs can generate text that is contextually appropriate and coherent.
- Few tasks where LLMs have improved the SOTA : 
  - Text translation
  - Sentiment analysis
  - Question answering
- LLMs are not trained for a specific task (unlike other NLP models):
  - They are trained on a large corpus of text data. 
  - They display broad capabilities across a range of tasks.
- LLMs started capturing wide variety of linguistic nuances and context when :
  - They were trained on large datasets.
  - They adopted transformer architecture.

## What is LLM ?
- LLM : Large Language Model
- "Large" because of number of parameters, and amount of data used to train the model.
- Training task : Predict next word in a sequence.
- Architecture : Transformer (helps them pay selective attention to different parts of the input during inference).
- LLMs comes under the domain of Generative AI, or GenAI. There are LLMS that are not GEnAi modles as well as GenAi models that are not LLMS ( Deep Learning models)
- LLMS are subset of DL, DL is a subset of ML, Ai encomposes all different models and systems with human like intelligence.

## Applications of LLMs
- Knowledge retrieval from vast volume of specialized texts.
- Code generation.
- Text translation.
- Sentiment analysis.
- Question answering.

## Stages of building and using LLMs
- Why build ?
  - Excellent exercise to understand its mechanics and limitations.
  - Equips us with the required knowledge for pretraining or fine-tuning existing open-source LLM architectures.
- Two-stage training :
  - Pre-training :
    - Task : Predict next word in a sequence. 
    - Training a LLM on a large, diverse dataset to develop a broad understanding of language. (unlabeled data)
    - The pre-trained model is referred as base or foundational model.
  - Fine-tuning : 
    - LLM is specifically trained on a dataset that is more specific to particular tasks or domains.
    - Popular type of fine-tuning : Instruction, Classification.
    - Instruction-tuning : Labeled dataset consists of instruction and answer pairs.
    - Classification-tuning : Labeled dataset consists of texts and associated class labels.
- What do we use next word prediction task for pre-training ?
  - Unlike traditional AI / ML models, the features are not engineered by the experts for deep learning models.
  - However, they do need labelled data.
  - Given a huge corpus of raw data (text), we can generate a data-set for this task programmatically.

## Introducing the transformer architecture
- Transformer architecture was proposed in the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762).
  - It was primarily designed for machine translation tasks.
  - It consists of an encoder and a decoder.
  - Both the encoder and decoder consist of many layers with self-attention mechanism.
  - Self-attention mechanism allows the model to weigh the importance of different tokens relative to each other.
- Encoder : 
  - Takes input text and pre-processes it for encoder.
  - Pre-processed text is encoded into a sequence of vectors.
  - Embedding captures the contextual information of the input text.
- Decoder :
  - Takes input text and pre-processes it for decoder.
  - Decoder takes pre-processed text and embedding generated by encoder to generate output text.
  - Decoder generates output text one word at a time.
- Transformer model proposed in `Attention is All You Need` paper is a sequence-to-sequence model.
  - Sequence-to-sequence models have an encoder and a decoder.
  - There are other variants of transformer models : Encoder-only, Decoder-only.
- BERT (Bidirectional Encoder Representations from Transformers) is another popular transformer model.
  - BERT uses the encoder part of the transformer architecture.
  - BERT is trained on Masked Language Model (MLM) task and Next Sentence Prediction (NSP) task.
  - Masked Language Model (MLM) task :
    - Randomly mask some percentage of the input tokens and predict them.
    - This task forces the model to learn the bidirectional context of the input text.
  - Next Sentence Prediction (NSP) task :
    - Given two sentences, predict whether the second sentence is the next sentence in the sequence.
    - This task helps the model understand the relationship between two sentences.
  - BERT is suitable for tasks like text classification, paraphrase detection, etc.
- GPT (Generative Pre-trained Transformer) is another popular transformer model.
  - GPT uses the decoder part of the transformer architecture.
  - GPT is trained on Casual Language Model (CLM) task.
  - Casual Language Model (CLM) task :
    - Given a sequence of tokens, predict the next token in the sequence (next word prediction).
    - This task helps the model generate coherent and contextually appropriate text one token at a time.
  - GPT is suitable for tasks like text generation.
- Not all transformers are LLMs and not all LLMs are transformers.
  - Transformers are a type of neural network architecture. 
  - LLMs are a type of model that can generate text.
  - There are vision transformers, speech transformers, etc.
  - LLMs can be based on RNNs, LSTMs, etc.

## Utilizing Large Datasets
- LLMs require large datasets to learn the nuances of language.
- The datasets should be diverse (cover a wide range of topics), high-quality, and large.
- For example, the GPT-3 model was trained on the following dataset :

| Dataset name | Dataset description        | Number of tokens | Proportion in training data |
|--------------|----------------------------|------------------|-----------------------------|
| CommonCrawl  | Web crawl data             | 410 billion      | 60%                         |
| WebText2     | Web crawl data             | 19 billion       | 22%                         |
| Books1       | Internet-based book corpus | 12 billion       | 8%                          |
| Books2       | Internet-based book corpus | 55 billion       | 8%                          |
| Wikipedia    | High-quality text          | 3 billion        | 3%                          |
- The authors of the GPT-3 paper did not share the training dataset.
  - A comparable dataset that is publicly available is [Dolma: an Open Corpus of Three Trillion Tokens for LLM Pretraining Research](https://arxiv.org/abs/2402.00159).
  - Books1 is likely a sample from Project Gutenberg, and Books2 is likely from Libgen.
  - Dolma dataset consists of 3 trillion tokens.
  - This dataset might contain copyrighted material.
  - GPT3 pretraining cost is estimated to be 4.6 million USD.

## A closer look at the GPT architecture
- GPT was introduced by OpenAI in the paper [Improving Language Understanding by Generative Pre-Training](http://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf).
- OpenAI also introduced InstructGPT in the paper [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155).
- Initial version of ChatGPT was created by fine-tuning GPT-3 on a large instruction dataset using a method from OpenAI's InstructGPT paper
- GPT models are pre-trained on a large dataset using the Casual Language Model (CLM) task.
  - Recap : CLM task involves predicting the next token in a sequence.
  - CLM is a form of self-supervised learning (i.e) a form of self-labeling.
  - We can create label's on-the-fly using next token in a sequence as the label.
- GPT models are decoder-only models.
  - They don't have an encoder.
  - They generate text one token at a time.
  - Since they include previous output as input for making future predictions, they are auto-regressive models.
- Emergent behavior : The ability to perform tasks that the model wasn't explicitly trained to perform.
  - This means the capability emerges as a consequence of the model's exposure to vast quantities of data in diverse contexts. 
  - For example : GPT models can "learn" the translation patterns between languages without being explicitly trained on translation tasks.

## Building a large language model
- Stage 1 : Building an LLM
  - Data preparation and sampling
  - Attention mechanism
  - LLM architecture
- Stage 2 : Foundational Model
  - Iterative training loop
  - Iterative model evaluation
  - Selecting model weights
- Stage 3 : Fine-tuning
  - Instruction-tuning
  - Classification-tuning