{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chap -2 Code Building a tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\mldl projects\\vproj\\lib\\site-packages (from tiktoken) (2024.7.24)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\mldl projects\\vproj\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\mldl projects\\vproj\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\mldl projects\\vproj\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\mldl projects\\vproj\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\mldl projects\\vproj\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Downloading tiktoken-0.8.0-cp312-cp312-win_amd64.whl (883 kB)\n",
      "   ---------------------------------------- 0.0/883.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 883.8/883.8 kB 10.0 MB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import os\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it'\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tokens using regex and converting it into token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']\n",
      "4690\n"
     ]
    }
   ],
   "source": [
    "tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "tokens = [t.strip() for t in tokens if t.strip()]\n",
    "print(tokens[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "words  = sorted(set(tokens))\n",
    "l = len(words)\n",
    "print(l)\n",
    "vocab = {token:integer for integer, token in enumerate(words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it all in class\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.int_to_str = {v:k for k,v in self.vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        tokens = [t.strip() for t in tokens if t.strip()]\n",
    "        return [self.vocab[token] for token in tokens]\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        text = \"\".join([self.int_to_str[token] for token in tokens])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53, 44, 149, 1003, 57, 38, 818, 115, 256, 486]\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "encoded = tokenizer.encode(text)\n",
    "print(encoded[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53, 44, 149, 1003, 57, 38, 818, 115, 256, 486]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code from book.\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "encoded = tokenizer.encode(text)\n",
    "print(encoded[:10])\n",
    "tokenizer.decode(encoded[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding special tokens\n",
    "vocab['<PAD>'] = l\n",
    "vocab['<UNK>'] = l+1\n",
    "vocab['<BOS>'] = l+2\n",
    "vocab['<EOS>'] = l+3\n",
    "l += 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1132, 53, 44, 149, 1003, 57, 38, 818, 115, 256]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<BOS> I HAD always thought Jack Gisburn rather a cheap genius -- though a good fellow enough -- so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera.( Though I rather thought it would have been Rome or Florence.)\" The height of his glory\" -- that was what the women called it. I can hear Mrs. Gideon Thwing -- his last Chicago sitter'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writing the tokenizer classs so that it knows when and how to use the special tokens\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text, add_bos=False, add_eos=False):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int.get(s, self.str_to_int['<UNK>']) for s in preprocessed]\n",
    "        if add_bos:\n",
    "            ids = [self.str_to_int['<BOS>']] + ids\n",
    "        if add_eos:\n",
    "            ids = ids + [self.str_to_int['<EOS>']]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "encoded = tokenizer.encode(text, add_bos=True, add_eos=True)\n",
    "print(encoded[:10])\n",
    "tokenizer.decode(encoded[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1132, 1131, 5, 355, 1126, 628, 975, 10, 1133, 55]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<BOS> <UNK>, do you like tea? <EOS> In the sunlit terraces of the <UNK>. <EOS>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <EOS> \".join((text1, text2))\n",
    "\n",
    "encoded = tokenizer.encode(text, add_bos=True, add_eos=True)\n",
    "print(encoded[:10])\n",
    "tokenizer.decode(encoded)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BytePair Encoding and Data Sampling\n",
    "using the tiktoken library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(text)\n",
    "print(len(enc_text))\n",
    "enc_sample  = enc_text[100:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5469, 438, 14363, 938, 4842, 1650, 353, 438]\n",
      "[438, 14363, 938, 4842, 1650, 353, 438, 2934]\n"
     ]
    }
   ],
   "source": [
    "context_size = 8\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wing ==> --\n",
      "wing-- ==> his\n",
      "wing--his ==>  last\n",
      "wing--his last ==>  Chicago\n",
      "wing--his last Chicago ==>  sit\n",
      "wing--his last Chicago sit ==> ter\n",
      "wing--his last Chicago sitter ==> --\n",
      "wing--his last Chicago sitter-- ==> de\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"==>\", tokenizer.decode([target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a simple data loader using pytorch, that iterates over the input dataset and returns the inputs and targets shifted by one\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, context_size=8,stride=1):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(text, allowed_special = {\"<EOS>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids)-context_size, stride):\n",
    "            self.input_ids.append(token_ids[i:i+context_size])\n",
    "            self.target_ids.append(token_ids[i+1:i+context_size+1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.input_ids[idx]), torch.LongTensor(self.target_ids[idx])\n",
    "    \n",
    "def create_data_loader(text, tokenizer, context_size=8, stride=128, batch_size=4):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = TextDataset(text, tokenizer, context_size, stride)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3347, 27846,   503,  2048,  4628, 24882,   379,   262],\n",
      "        [  257,  2726,  6227,   284,  1833,   683,  1365,    13],\n",
      "        [  286,   616,  4286,   705,  1014,   510,    26,   475],\n",
      "        [   11,   508,   550, 18459,  1068,   284,  1577,   257],\n",
      "        [ 2612,  4369,    11,   523,   326,   612,   550,   587],\n",
      "        [ 1021,   757,   438, 10919,   257,   410,  5040,   329],\n",
      "        [  616,   705, 23873,  2350,     6, 14707,   588,   257],\n",
      "        [  314,   550,  1775,   683,    11,   523,  1690,    11]])\n",
      "tensor([[27846,   503,  2048,  4628, 24882,   379,   262,  8812],\n",
      "        [ 2726,  6227,   284,  1833,   683,  1365,    13,   198],\n",
      "        [  616,  4286,   705,  1014,   510,    26,   475,   314],\n",
      "        [  508,   550, 18459,  1068,   284,  1577,   257, 23844],\n",
      "        [ 4369,    11,   523,   326,   612,   550,   587,   645],\n",
      "        [  757,   438, 10919,   257,   410,  5040,   329,   257],\n",
      "        [  705, 23873,  2350,     6, 14707,   588,   257,  2156],\n",
      "        [  550,  1775,   683,    11,   523,  1690,    11,  1615]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_data_loader(text, tokenizer, context_size=8, stride=128, batch_size=8)\n",
    "for x, y in dataloader:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating token embeddings and encoding word positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using torch embedding\n",
    "vocab_size = len(tokenizer.encoder)\n",
    "embedding_dim = 4\n",
    "torch.manual_seed(0)\n",
    "embedding = torch.nn.Embedding(vocab_size, embedding_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
