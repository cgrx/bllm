{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Working with Text Data",
   "id": "9ffea3583fa41e48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:19.737882Z",
     "start_time": "2025-01-11T05:56:18.311455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "import tiktoken\n",
    "from torch import arange, manual_seed, nn, tensor, Tensor\n",
    "from torch.utils.data import DataLoader, Dataset"
   ],
   "id": "414f61f2cbb7566c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:19.752613Z",
     "start_time": "2025-01-11T05:56:19.744380Z"
    }
   },
   "cell_type": "code",
   "source": "manual_seed(123)",
   "id": "f8c0ed9a7a23da4d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x70a780d19750>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenizing Text",
   "id": "73ec76b8996a027"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:19.908655Z",
     "start_time": "2025-01-11T05:56:19.904731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load text from a file\n",
    "\n",
    "    Args:\n",
    "        - file_path (str): path to the file\n",
    "\n",
    "    Returns:\n",
    "        - text (str): text from the file\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text"
   ],
   "id": "86b67262e257763a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:19.957498Z",
     "start_time": "2025-01-11T05:56:19.951627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the text\n",
    "raw_text = load_text(\"../data/the-verdict.txt\")\n",
    "print(f\"Total number of character: {len(raw_text)}\")"
   ],
   "id": "d7620f0d654610cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:20.010534Z",
     "start_time": "2025-01-11T05:56:20.003588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess the text and split into tokens\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(f\"Total number of tokens: {len(preprocessed)}\")"
   ],
   "id": "d4903917540fcbc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 4690\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:20.067115Z",
     "start_time": "2025-01-11T05:56:20.059934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Identify unique tokens to build vocabulary\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ],
   "id": "e6070e082f08ab2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1130\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:20.123954Z",
     "start_time": "2025-01-11T05:56:20.114978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"\n",
    "    A simple tokenizer that converts text to token IDs and vice versa.\n",
    "\n",
    "    Attributes:\n",
    "        - token_to_id (Dict[str, int]): mapping from token to ID\n",
    "        - id_to_token (Dict[int, str]): mapping from ID to token\n",
    "\n",
    "    Note:\n",
    "        - This one includes code from V1 and V2 of SimpleTokenizer in the book\n",
    "    \"\"\"\n",
    "    def __init__(self, words: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer\n",
    "\n",
    "        Args:\n",
    "            - words (List[str]): list of words in the text\n",
    "        \"\"\"\n",
    "        for special_token in [\"<|endoftext|>\", \"<|unk|>\"]:\n",
    "            if special_token not in words:\n",
    "                words.append(special_token)\n",
    "        self.token_to_id = {token:integer for integer,token in enumerate(words)}\n",
    "        self.id_to_token = {integer: token for token, integer in self.token_to_id.items()}\n",
    "\n",
    "    def encode(self, input_text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Convert text to token IDs\n",
    "\n",
    "        Args:\n",
    "            - input_text (str): input text\n",
    "\n",
    "        Returns:\n",
    "            - List[int]: list of token IDs\n",
    "        \"\"\"\n",
    "        pre_processed_text = re.split(r'([,.?_!\"()\\']|--|\\s)', input_text)\n",
    "        pre_processed_text = [item.strip() for item in pre_processed_text if item.strip()]\n",
    "        # Replace OOV words with <|unk|>\n",
    "        pre_processed_text = [item if item in self.token_to_id else \"<|unk|>\" for item in pre_processed_text]\n",
    "        ids = [self.token_to_id[s] for s in pre_processed_text]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Convert token IDs back to text\n",
    "\n",
    "        Args:\n",
    "            - ids (List[int]): list of token IDs\n",
    "\n",
    "        Returns:\n",
    "            - str: text\n",
    "        \"\"\"\n",
    "        text = \" \".join([self.id_to_token[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)     # Remove space before punctuation\n",
    "        return text\n",
    "\n",
    "tokenizer = SimpleTokenizer(all_words)"
   ],
   "id": "da920b9a94436b29",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:20.175798Z",
     "start_time": "2025-01-11T05:56:20.170266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ],
   "id": "a4e8b66e9f6c52f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:20.228536Z",
     "start_time": "2025-01-11T05:56:20.223738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)"
   ],
   "id": "c4b39654466f7bb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:20.281521Z",
     "start_time": "2025-01-11T05:56:20.276349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Case - Just OOV words\n",
    "ids = tokenizer.encode(input_text=\"Hello, do you like tea?\")\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)"
   ],
   "id": "9ad5774c2f9cf4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea?\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:20.336750Z",
     "start_time": "2025-01-11T05:56:20.330746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Case - OOV words and special tokens\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "ids = tokenizer.encode(text)\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)"
   ],
   "id": "c3e241179f7c6552",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Byte Pair Encoding",
   "id": "cd32e464070c043a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:20.661579Z",
     "start_time": "2025-01-11T05:56:20.384922Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = tiktoken.get_encoding(\"gpt2\")",
   "id": "8498da81dc47cfbb",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:20.680221Z",
     "start_time": "2025-01-11T05:56:20.676639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of the someunkownPalace.\"\n",
    "ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(ids)"
   ],
   "id": "2df7975410c64d18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 617, 2954, 593, 11531, 558, 13]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:20.724009Z",
     "start_time": "2025-01-11T05:56:20.718389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)"
   ],
   "id": "66c651972e699ee5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the someunkownPalace.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:20.778134Z",
     "start_time": "2025-01-11T05:56:20.771843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unknown_word = \"Akwirw ier\"\n",
    "ids = tokenizer.encode(unknown_word)\n",
    "print(f\"Token IDs for unknown word: {ids}\")\n",
    "for entry in ids:\n",
    "    decoded_text = tokenizer.decode([entry])\n",
    "    print(f\"ID: {entry}, Token: {decoded_text}\")"
   ],
   "id": "ca8167dd4a35e550",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs for unknown word: [33901, 86, 343, 86, 220, 959]\n",
      "ID: 33901, Token: Ak\n",
      "ID: 86, Token: w\n",
      "ID: 343, Token: ir\n",
      "ID: 86, Token: w\n",
      "ID: 220, Token:  \n",
      "ID: 959, Token: ier\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Sampling with a Sliding Window",
   "id": "87ec78bf6927081"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:20.831766Z",
     "start_time": "2025-01-11T05:56:20.824773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for text data\n",
    "\n",
    "    Attributes:\n",
    "        - input_ids (List[Tensor]): List of input token IDs\n",
    "        - target_ids (List[Tensor]): List of target token IDs\n",
    "    \"\"\"\n",
    "    def __init__(self, txt: str, tokenizer: tiktoken.core.Encoding, max_length: int, stride: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "\n",
    "        Args:\n",
    "            - txt (str): Text data\n",
    "            - tokenizer (tiktoken.core.Encoding): Tokenizer object from TikToken\n",
    "            - max_length (int): Maximum length of the input sequence\n",
    "            - stride (int): Stride for the sliding window\n",
    "        \"\"\"\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(tensor(input_chunk))\n",
    "            self.target_ids.append(tensor(target_chunk))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the number of samples\n",
    "\n",
    "        Returns:\n",
    "            - int: Number of samples\n",
    "        \"\"\"\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset\n",
    "\n",
    "        Args:\n",
    "            - idx (int): Index of the sample\n",
    "\n",
    "        Returns:\n",
    "            - Tuple[Tensor, Tensor]: input and target token IDs\n",
    "        \"\"\"\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ],
   "id": "a3b7ff185071cbfe",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:20.883378Z",
     "start_time": "2025-01-11T05:56:20.877472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataloader_v1(txt: str, batch_size: int = 4, max_length: int = 256, stride: int = 128, shuffle:bool = True, drop_last:bool = True) -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create a PyTorch DataLoader for text data\n",
    "\n",
    "    Args:\n",
    "        - txt (str): Text data\n",
    "        - batch_size (int): Batch size\n",
    "        - max_length (int): Maximum length of the input sequence\n",
    "        - stride (int): Stride for the sliding window\n",
    "        - shuffle (bool): Shuffle the data\n",
    "        - drop_last (bool): Drop the last incomplete batch\n",
    "\n",
    "    Returns:\n",
    "        - DataLoader: PyTorch DataLoader\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "    return dataloader"
   ],
   "id": "1d8ab654e3f574f5",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:20.933059Z",
     "start_time": "2025-01-11T05:56:20.928603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the text\n",
    "raw_text = load_text(\"../data/the-verdict.txt\")"
   ],
   "id": "5072bfa2e6fdc827",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:20.993310Z",
     "start_time": "2025-01-11T05:56:20.979387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a DataLoader and get the first batch\n",
    "dataloader = create_dataloader_v1(txt=raw_text, batch_size=8, max_length=256, stride=256)\n",
    "inputs, targets = next(iter(dataloader))\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")"
   ],
   "id": "1c65a1548ff931b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([8, 256])\n",
      "Target shape: torch.Size([8, 256])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating Token Embeddings and Encoding Positional Information",
   "id": "37c86f5f10b89713"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:21.043877Z",
     "start_time": "2025-01-11T05:56:21.037938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "output_dim = 768    # Dimension of the output embeddings in GPT-2\n",
    "context_length = inputs.shape[1]\n",
    "print(f\"Vocabulary size: {vocab_size}, Output dimension: {output_dim}, Context length: {context_length}\")"
   ],
   "id": "88d314eff30e87ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257, Output dimension: 768, Context length: 256\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:21.346864Z",
     "start_time": "2025-01-11T05:56:21.090629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create token embeddings to map token IDs to a dense vector representation\n",
    "token_embedding_layer = nn.Embedding(vocab_size, output_dim)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(f\"Token embeddings shape: {token_embeddings.shape}\")"
   ],
   "id": "b968f712621e2e6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings shape: torch.Size([8, 256, 768])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:21.388718Z",
     "start_time": "2025-01-11T05:56:21.382089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create positional embeddings to add positional awareness\n",
    "positional_embedding_layer = nn.Embedding(context_length, output_dim)\n",
    "positional_embeddings = positional_embedding_layer(arange(context_length))\n",
    "print(f\"Positional embeddings shape: {positional_embeddings.shape}\")"
   ],
   "id": "20abbe19c79736d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embeddings shape: torch.Size([256, 768])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:56:21.444277Z",
     "start_time": "2025-01-11T05:56:21.433047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add token and positional embeddings to create input embeddings.\n",
    "# Positional embeddings are broadcasted across the batch dimension.\n",
    "input_embeddings = token_embeddings + positional_embeddings\n",
    "print(f\"Input embeddings shape: {input_embeddings.shape}\")"
   ],
   "id": "3facfa578a65d71c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings shape: torch.Size([8, 256, 768])\n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
