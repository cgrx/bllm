{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstructing GPT4 MinBPE Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "Write the BasicTokenizer class, with the following three core functions:\n",
    "\n",
    "def train(self, text, vocab_size, verbose=False)\n",
    "def encode(self, text)\n",
    "def decode(self, ids)\n",
    "Train your tokenizer on whatever text you like and visualize the merged tokens. Do they look reasonable? One default test you may wish to use is the text file tests/taylorswift.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a BPE Tokenizer class based on the functions defined above\n",
    "\n",
    "class BPE_TokenizerV1:\n",
    "    def __init__(self, vocab_size, num_merges):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_merges = num_merges\n",
    "        self.merges = None\n",
    "        self.vocab = None\n",
    "\n",
    "    def stats(self, ids):\n",
    "        count = {}\n",
    "        for pair in zip(ids,ids[1:]):\n",
    "            count[pair] = count.get(pair,0) + 1 # counting the frequency of each pair\n",
    "        return count\n",
    "\n",
    "    def replace_pair(self, ids, pair, new_id):\n",
    "        new_ids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "                new_ids.append(new_id)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_ids.append(ids[i])\n",
    "                i += 1\n",
    "        return new_ids\n",
    "\n",
    "    def bpe_train(self, ids):\n",
    "        merges = {}\n",
    "        for i in range(self.num_merges):\n",
    "            stat = self.stats(ids)\n",
    "            top_pair = max(stat, key=stat.get)\n",
    "            idx = self.vocab_size + i\n",
    "            print(f\"merging {top_pair} to {idx}\")\n",
    "            ids = self.replace_pair(ids, top_pair, idx)\n",
    "            merges[top_pair] = idx\n",
    "        self.merges = merges\n",
    "        self.ids = ids\n",
    "        vocab = {idx:bytes([idx]) for idx in range(256)}\n",
    "        for pair, idx in merges.items():\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "        self.vocab = vocab\n",
    "        return ids, merges\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode('utf-8'))\n",
    "        while len(tokens) >= 2:\n",
    "            stat = self.stats(tokens)\n",
    "            pair = min(stat, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            tokens = self.replace_pair(tokens, pair, idx)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = b\"\".join([self.vocab[t] for t in tokens]) #b\"\" is used to convert the string to bytes\n",
    "        return text.decode('utf-8', errors='replace') # replace any unknown characters with a replacement character\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "tokenizer = BPE_TokenizerV1(256, 20)\n",
    "ids = list(tokens2)\n",
    "ids, merges = tokenizer.bpe_train(ids)\n",
    "print(\"tokens length before BPE:\", len(tokens2))\n",
    "print(\"tokens length after BPE:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens2)/len(ids):.2f}\")\n",
    "print(tokenizer.decode(tokenizer.encode(\"hello world\")))\n",
    "text2 = tokenizer.decode(tokenizer.encode(text))\n",
    "print(text2 == text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "Convert you BasicTokenizer into a RegexTokenizer, which takes a regex pattern and splits the text exactly as GPT-4 would. Process the parts separately as before, then concatenate the results. Retrain your tokenizer and compare the results before and after. You should see that you will now have no tokens that go across categories (numbers, letters, punctuation, more than one whitespace). Use the GPT-4 pattern:\n",
    "\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
